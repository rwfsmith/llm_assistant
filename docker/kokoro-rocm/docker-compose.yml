# docker-compose.yml – Kokoro TTS API (ONNX Runtime) with ROCm (AMD GPU) support
#
# Quick start:
#   export VIDEO_GID=$(getent group video | cut -d: -f3)
#   export RENDER_GID=$(getent group render | cut -d: -f3)
#   docker compose up -d --build
#
# The container downloads the ONNX model (~169 MB for fp16) on first start.
# CPU-only build: docker compose build --build-arg GPU_SUPPORT=cpu

name: kokoro-rocm

services:
  kokoro-tts:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        ROCM_VERSION: "7.2"
        GPU_SUPPORT: ${GPU_SUPPORT:-rocm}

    container_name: kokoro-tts-rocm

    restart: unless-stopped

    # Run as root so the container can write to the host-mounted volume.
    # The Dockerfile creates appuser (uid 1001) but mounted dirs are root-owned.
    user: root

    ports:
      - "8880:8880"

    # --- AMD GPU passthrough ---
    devices:
      - /dev/dri   # GPU render nodes (e.g. /dev/dri/renderD128)
      - /dev/kfd   # ROCm kernel fusion driver

    group_add:
      # Group IDs for: video (44), render (109 on most distros – check with
      # `getent group render | cut -d: -f3`). Override with env vars if needed.
      - "${VIDEO_GID:-44}"
      - "${RENDER_GID:-109}"

    environment:
      - PYTHONUNBUFFERED=1
      # ONNX model variant: fp32 | fp16 (default) | fp16-gpu | int8
      - MODEL_VARIANT=${MODEL_VARIANT:-fp16}
      # Execution provider: ROCMExecutionProvider | CPUExecutionProvider | auto
      - ONNX_PROVIDER=${ONNX_PROVIDER:-ROCMExecutionProvider}
      # MIOpen kernel tuning – required on first run for GPUs without pre-built kdb.
      # After first successful startup, consider changing to MIOPEN_FIND_MODE=2.
      - MIOPEN_FIND_MODE=3
      - MIOPEN_FIND_ENFORCE=3

    volumes:
      # Model weights – avoids re-downloading on container recreate
      - ./data/models:/app/models
      # MIOpen kernel shape cache (speeds up inference after first run)
      - ./data/miopen-config:/root/.config/miopen
      - ./data/miopen-cache:/root/.cache/miopen

    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

    # Optional: limit to a specific GPU by PCI bus ID
    # Uncomment and adjust if you have multiple AMD GPUs:
    # environment:
    #   - ROCR_VISIBLE_DEVICES=0

  # ---------------------------------------------------------------------------
  # Wyoming protocol proxy – exposes Kokoro-FastAPI as a Wyoming TTS service
  # so Home Assistant can use it via the built-in Wyoming integration on
  # port 10200 (TCP), in addition to the OpenAI-compatible HTTP API.
  # ---------------------------------------------------------------------------
  kokoro-wyoming:
    build:
      context: ./wyoming
      dockerfile: Dockerfile

    container_name: kokoro-wyoming

    restart: unless-stopped

    ports:
      - "10200:10200"

    depends_on:
      kokoro-tts:
        condition: service_healthy

    environment:
      # URL of the Kokoro-FastAPI service (internal Docker network name)
      - KOKORO_URL=http://kokoro-tts:8880
      # Default voice when the client does not request a specific one
      - DEFAULT_VOICE=${KOKORO_DEFAULT_VOICE:-af_bella}
      - DEFAULT_SPEED=${KOKORO_DEFAULT_SPEED:-1.0}

    command: >
      --kokoro-url http://kokoro-tts:8880
      --voice ${KOKORO_DEFAULT_VOICE:-af_bella}
      --speed ${KOKORO_DEFAULT_SPEED:-1.0}
